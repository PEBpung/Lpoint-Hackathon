{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wide & Deep Learning model 구현"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 라이브러리 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import os\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import plot_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clnt_id</th>\n",
       "      <th>sess_id</th>\n",
       "      <th>hit_seq</th>\n",
       "      <th>action_type</th>\n",
       "      <th>biz_unit</th>\n",
       "      <th>sess_dt</th>\n",
       "      <th>hit_tm</th>\n",
       "      <th>hit_pss_tm</th>\n",
       "      <th>trans_id</th>\n",
       "      <th>sech_kwd</th>\n",
       "      <th>...</th>\n",
       "      <th>latest_act_hr_1</th>\n",
       "      <th>latest_act_hr_2</th>\n",
       "      <th>latest_act_hr_3</th>\n",
       "      <th>hum</th>\n",
       "      <th>temp</th>\n",
       "      <th>pty</th>\n",
       "      <th>r06</th>\n",
       "      <th>clnt_gender</th>\n",
       "      <th>clnt_age</th>\n",
       "      <th>clac_nm2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>121082</th>\n",
       "      <td>3390</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>A01</td>\n",
       "      <td>2019-07-01</td>\n",
       "      <td>00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>지고트</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>59.0</td>\n",
       "      <td>24.700001</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194580</th>\n",
       "      <td>5535</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>A03</td>\n",
       "      <td>2019-07-01</td>\n",
       "      <td>00:00</td>\n",
       "      <td>6532</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>59.0</td>\n",
       "      <td>24.700001</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>F</td>\n",
       "      <td>30.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194581</th>\n",
       "      <td>5535</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>A03</td>\n",
       "      <td>2019-07-01</td>\n",
       "      <td>00:00</td>\n",
       "      <td>30494</td>\n",
       "      <td>NaN</td>\n",
       "      <td>양파</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>59.0</td>\n",
       "      <td>24.700001</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>F</td>\n",
       "      <td>30.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194582</th>\n",
       "      <td>5535</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>A03</td>\n",
       "      <td>2019-07-01</td>\n",
       "      <td>00:00</td>\n",
       "      <td>32370</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>59.0</td>\n",
       "      <td>24.700001</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>F</td>\n",
       "      <td>30.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194583</th>\n",
       "      <td>5535</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>A03</td>\n",
       "      <td>2019-07-01</td>\n",
       "      <td>00:00</td>\n",
       "      <td>41637</td>\n",
       "      <td>NaN</td>\n",
       "      <td>우엉</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>59.0</td>\n",
       "      <td>24.700001</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>F</td>\n",
       "      <td>30.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 40 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        clnt_id  sess_id  hit_seq  action_type biz_unit     sess_dt hit_tm  \\\n",
       "121082     3390        1        1            0      A01  2019-07-01  00:00   \n",
       "194580     5535        1        1            5      A03  2019-07-01  00:00   \n",
       "194581     5535        1        2            0      A03  2019-07-01  00:00   \n",
       "194582     5535        1        3            3      A03  2019-07-01  00:00   \n",
       "194583     5535        1        4            0      A03  2019-07-01  00:00   \n",
       "\n",
       "        hit_pss_tm  trans_id sech_kwd  ...  latest_act_hr_1  latest_act_hr_2  \\\n",
       "121082           0       NaN      지고트  ...             -1.0             -1.0   \n",
       "194580        6532       NaN      NaN  ...             -1.0             -1.0   \n",
       "194581       30494       NaN       양파  ...             -1.0             -1.0   \n",
       "194582       32370       NaN      NaN  ...             -1.0             -1.0   \n",
       "194583       41637       NaN       우엉  ...             -1.0             -1.0   \n",
       "\n",
       "       latest_act_hr_3   hum       temp  pty  r06  clnt_gender  clnt_age  \\\n",
       "121082            -1.0  59.0  24.700001  0.0  0.0          NaN       NaN   \n",
       "194580            -1.0  59.0  24.700001  0.0  0.0            F      30.0   \n",
       "194581            -1.0  59.0  24.700001  0.0  0.0            F      30.0   \n",
       "194582            -1.0  59.0  24.700001  0.0  0.0            F      30.0   \n",
       "194583            -1.0  59.0  24.700001  0.0  0.0            F      30.0   \n",
       "\n",
       "        clac_nm2  \n",
       "121082       NaN  \n",
       "194580       NaN  \n",
       "194581       NaN  \n",
       "194582       NaN  \n",
       "194583       NaN  \n",
       "\n",
       "[5 rows x 40 columns]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df3_merge = pd.read_csv('./data/df3_merge.csv')\n",
    "df3_merge = df3_merge.sort_values(['sess_dt', 'hit_tm'])\n",
    "df3_merge.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "구매 이력이 있는 행: 3.88 %\n"
     ]
    }
   ],
   "source": [
    "print('구매 이력이 있는 행:', round(df3_merge['clac_nm2'].notna().sum() / df3_merge['clac_nm2'].isna().sum() * 100, 2),'%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "user2vec = pd.read_csv('./data/user2vec.csv')\n",
    "df_buyer = df3_merge[df3_merge['clac_nm2'].notna()]\n",
    "df_buyer = df_buyer.merge(user2vec, on=['clnt_id', 'sess_id'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_col=['clnt_id', 'sess_id', 'hit_seq', 'action_type', 'hit_tm', 'trans_id', 'sech_kwd']\n",
    "df_buyer = df_buyer.drop(drop_col, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "\n",
    "lab_len = len(df_buyer['clac_nm2'].value_counts())\n",
    "label_key = df_buyer['clac_nm2'].value_counts().keys()\n",
    "\n",
    "df_buyer['clac_nm2'] = le.fit_transform(df_buyer['clac_nm2'])\n",
    "clac_nm2 = df_buyer['clac_nm2'].copy()\n",
    "df_buyer.drop(['clac_nm2'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_buyer.loc[df_buyer['pv_hr'].isnull(),'pv_hr'] = 0\n",
    "df_buyer.loc[df_buyer['latest_pv_hr_1'].isnull(),'latest_pv_hr_1'] = 0\n",
    "df_buyer.loc[df_buyer['latest_pv_hr_2'].isnull(),'latest_pv_hr_2'] = 0\n",
    "df_buyer.loc[df_buyer['latest_pv_hr_3'].isnull(),'latest_pv_hr_3'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df_buyer.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "ALL_CATEGORICAL_COLUMNS = [\n",
    "    \"biz_unit\", \"sess_dt\", \"tot_pag_view_ct\", 'trfc_src', 'trfc_src', 'dvc_ctg_nm',\n",
    "    'cum_act_0', 'cum_act_1', 'cum_act_2', 'cum_act_3', 'cum_act_4', 'cum_act_5', 'cum_act_6',\n",
    "    'cum_act_7', 'day', 'holiday', 'hour',  'prefer_dvc_trfc', 'clnt_gender',\n",
    "    'clnt_age', 'pty', 'r06', 'latest_act_hr_1', 'latest_act_hr_2', 'latest_act_hr_3',\n",
    "    'X_0', 'X_1', 'X_2', 'X_3', 'X_4', 'X_5', 'X_6',\n",
    "       'X_7', 'X_8', 'X_9', 'X_10', 'X_11', 'X_12', 'X_13', 'X_14', 'X_15',\n",
    "       'X_16', 'X_17', 'X_18', 'X_19', 'X_20', 'X_21', 'X_22', 'X_23', 'X_24',\n",
    "       'X_25', 'X_26', 'X_27', 'X_28', 'X_29'\n",
    "]\n",
    "\n",
    "CATEGORICAL_COLUMNS = [\n",
    "    \"biz_unit\", \"sess_dt\", \"tot_pag_view_ct\", 'trfc_src', 'trfc_src', 'dvc_ctg_nm',\n",
    "    'cum_act_0', 'cum_act_1', 'cum_act_2', 'cum_act_3', 'cum_act_4', 'cum_act_5', 'cum_act_6',\n",
    "    'cum_act_7', 'day', 'holiday', 'hour',  'prefer_dvc_trfc', 'clnt_gender',\n",
    "    'clnt_age', 'pty', 'r06', 'latest_act_hr_1', 'latest_act_hr_2', 'latest_act_hr_3'\n",
    "]\n",
    "\n",
    "EMBEDDED_COLUMNS = [\n",
    "    'X_0', 'X_1', 'X_2', 'X_3', 'X_4', 'X_5', 'X_6',\n",
    "       'X_7', 'X_8', 'X_9', 'X_10', 'X_11', 'X_12', 'X_13', 'X_14', 'X_15',\n",
    "       'X_16', 'X_17', 'X_18', 'X_19', 'X_20', 'X_21', 'X_22', 'X_23', 'X_24',\n",
    "       'X_25', 'X_26', 'X_27', 'X_28', 'X_29'\n",
    "]\n",
    "\n",
    "CONTINUOUS_COLUMNS = ['latest_pv_hr_3','latest_pv_hr_2', 'hit_pss_tm', 'pv_hr', 'tot_sess_hr_v', 'latest_pv_hr_1', 'temp', 'hum']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in CATEGORICAL_COLUMNS:\n",
    "    le = LabelEncoder()\n",
    "    data[c] = le.fit_transform(data[c])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train 데이터, Test 데이터 분할"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = clac_nm2.copy()\n",
    "label = np.eye(lab_len)[label]\n",
    "label = pd.DataFrame(data=label, columns=label_key, index=df_buyer.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x, test_x , train_y , test_y = train_test_split(data , label , test_size=0.05, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train 데이터:  (116940, 62)\n",
      "Test 데이터:  (6155, 62)\n",
      "Train 라벨:  (116940, 288)\n",
      "Test 라벨:  (6155, 288)\n"
     ]
    }
   ],
   "source": [
    "print('Train 데이터: ', train_x.shape)\n",
    "print('Test 데이터: ', test_x.shape)\n",
    "print('Train 라벨: ', train_y.shape)\n",
    "print('Test 라벨: ', test_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x, val_x , train_y , val_y = train_test_split(train_x , train_y , test_size=0.1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train 데이터:  (105246, 62)\n",
      "Val 데이터:  (11694, 62)\n",
      "Train 라벨:  (105246, 288)\n",
      "Val 라벨:  (11694, 288)\n"
     ]
    }
   ],
   "source": [
    "print('Train 데이터: ', train_x.shape)\n",
    "print('Val 데이터: ', val_x.shape)\n",
    "print('Train 라벨: ', train_y.shape)\n",
    "print('Val 라벨: ', val_y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 카테고리 값들과 연속값들을 뽑아냄"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x_category = np.array(train_x[CATEGORICAL_COLUMNS])\n",
    "test_x_category  = np.array(test_x[CATEGORICAL_COLUMNS])\n",
    "val_x_category   = np.array(val_x[CATEGORICAL_COLUMNS])\n",
    "\n",
    "train_x_embedding = np.array(train_x[EMBEDDED_COLUMNS])\n",
    "test_x_embedding  = np.array(test_x[EMBEDDED_COLUMNS])\n",
    "val_x_embedding   = np.array(val_x[EMBEDDED_COLUMNS])\n",
    "\n",
    "train_x_continue = np.array(train_x[CONTINUOUS_COLUMNS], dtype='float64')\n",
    "test_x_continue = np.array(test_x[CONTINUOUS_COLUMNS], dtype='float64')\n",
    "val_x_continue = np.array(val_x[CONTINUOUS_COLUMNS], dtype='float64')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 정규화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "train_x_continue = scaler.fit_transform(train_x_continue)\n",
    "test_x_continue = scaler.transform(test_x_continue)\n",
    "val_x_continue = scaler.transform(val_x_continue)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 정규화 내용 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-4.1421630839022345\n",
      "-1.8276499320635635\n",
      "-3.8417757572076456\n",
      "-3.840515541461756\n",
      "-4.559329867801354\n"
     ]
    }
   ],
   "source": [
    "print(train_x_continue[0].sum())\n",
    "print(train_x_continue[1].sum())\n",
    "print(train_x_continue[2].sum())\n",
    "print(train_x_continue[3].sum())\n",
    "print(train_x_continue[4].sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - 정규화 한다고 다 더해서 1이 되거나 하지는 않는듯\n",
    " - 그렇다고 개별 값이 0~1은 아님"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Polynomial 하게 바꿔줌 \n",
    "### (비선형적인 설정으로 선형 회귀를 확장하는 방법. 즉 다항식 함수로 바꿔줌)\n",
    "    - 카테고리 값을 Polynomial로 바꿔줌"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* sklearn.preprocessing.PolynomialFeatures 메소드\n",
    "    - degree : 다항식 차수\n",
    "    - interaction_only\n",
    "        - default는 False\n",
    "        - ex) degree = 3일 때, interaction_only=false 이면\n",
    "            - a^2, a^3, b^2, b^3, ab, a^2*b, ab^2 Feature가 추가되고,\n",
    "        - interaction_only=True 이면\n",
    "            - ab만 추가됨\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "poly = PolynomialFeatures(degree=2, interaction_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x_category_poly = poly.fit_transform(train_x_continue)\n",
    "test_x_category_poly = poly.fit_transform(test_x_continue)\n",
    "val_x_category_poly = poly.fit_transform(val_x_continue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(105246, 37)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x_category_poly.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " * np.unique : np.arr 내 중복 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.python.keras.layers.advanced_activations import ReLU, PReLU, LeakyReLU, ELU\n",
    "from tensorflow.python.keras.optimizers import Adam, SGD\n",
    "from tensorflow.python.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.python.keras.models import Model\n",
    "import tensorflow.keras.backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_deep_model():\n",
    "    \n",
    "    category_inputs = []\n",
    "    category_embeds = []\n",
    "    \n",
    "    # Categorical Data Embedding\n",
    "    for i in range(len(CATEGORICAL_COLUMNS)):\n",
    "        \n",
    "        # input - embedding - flatten 순으로 layer 쌓기\n",
    "        input_i = Input(shape=(1,), dtype='int32')\n",
    "        \n",
    "        dim = len(np.unique(data[CATEGORICAL_COLUMNS[i]]))\n",
    "        # dim : data에서 카테고리별 요소가 몇 종류인지?\n",
    "        \n",
    "        embed_dim = int(np.ceil(dim ** 0.5))\n",
    "        # embedding 차원을 0.5배 정도로 수행? (연산은 루트 올림인디?)\n",
    "        # 왜 임베딩 차원을 이렇게 하는지 추후 검토\n",
    "        \n",
    "        embed_i = Embedding(dim, embed_dim, input_length=1)(input_i)\n",
    "        # dim : 데이터가 몇 종류 있는지 = 임베딩 벡터를 몇 개 뽑아낼 것인지\n",
    "        # embed_dim : 임베딩 처리 후 벡터의 차원 = 임베딩 벡터를 몇 차원 벡터로 뽑아 낼 것인지\n",
    "        # input_length : 입력 데이터 길이\n",
    "        \n",
    "        flatten_i = Flatten()(embed_i)\n",
    "        # category 값을 임베딩환 벡터들을 flatten\n",
    "        # 서로 다른 값의 벡터 요소가 합쳐져도 괜찮은지??\n",
    "        # 어차피 class에 대한 순서가 있으니 상관 없을듯\n",
    "        \n",
    "        category_inputs.append(input_i)\n",
    "        category_embeds.append(flatten_i)\n",
    "        \n",
    "    # continuous 데이터 input\n",
    "    continue_input = Input(shape=(len(CONTINUOUS_COLUMNS),))\n",
    "    continue_dense = Dense(256, use_bias=False)(continue_input)\n",
    "    # use_bias = False로 하는 이유는??\n",
    "    \n",
    "    # category와 continue를 합침\n",
    "    concat_embeds = concatenate([continue_dense] + category_embeds)\n",
    "    concat_embeds = Activation('relu')(concat_embeds)\n",
    "    # Activation 효과 다시 공부\n",
    "    # relu 말고 다른 것은 어떤지??\n",
    "    bn_concat = BatchNormalization()(concat_embeds)\n",
    "    # Batch Normalization 효과 다시 공부\n",
    "    \n",
    "    fc1 = Dense(512, use_bias=False)(bn_concat)\n",
    "    relu1 = ReLU()(fc1)\n",
    "    bn1 = BatchNormalization()(relu1)\n",
    "    fc2 = Dense(256, use_bias=False)(bn1)\n",
    "    relu2 = ReLU()(fc2)\n",
    "    bn2 = BatchNormalization()(relu2)\n",
    "    fc3 = Dense(128)(bn2)\n",
    "    relu3 = ReLU()(fc3)\n",
    "    \n",
    "    return category_inputs, continue_input, relu3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wide_model(poly):\n",
    "    dim = poly.shape[1]\n",
    "    return tf.keras.layers.Input(shape=(dim,))\n",
    "\n",
    "# x_train_category_poly : 카테고리 데이터를 숫자로 바꾸고, Poly Feature를 추가한 것\n",
    "# Poly Feature : a, b, c Feature를 이용해서 ab, bc, ca Feature를 만든것\n",
    "# 데이터의 shape 만 가져옴"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embed_model(embed):\n",
    "    dim = embed.shape[1]\n",
    "    return tf.keras.layers.Input(shape=(dim,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    * input - embedding - flatten 순으로 layer 쌓기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_inputs, continue_input, deep_model = get_deep_model()\n",
    "wide_model = get_wide_model(train_x_category_poly)\n",
    "embed_model = get_embed_model(train_x_embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wide모델과 Deep model을 합치기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* wide model, deep model 합치기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_layer = concatenate([deep_model, wide_model, embed_model])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = [continue_input] + category_inputs + [wide_model] + [embed_model]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = Dense(len(label_key), activation='sigmoid')(out_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(inputs=inputs, outputs=output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 입력 데이터\n",
    "\n",
    "    * 위에서 정의한 리스트 변수 inputs에 맞추어\n",
    "    * continue 데이터 => category 데이터 => poly data 순으로 입력 값을 넣어준다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = [train_x_continue] + [train_x_category[:, i] for i in range(train_x_category.shape[1])] + [train_x_category_poly] + [train_x_embedding]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_data = [val_x_continue] + [val_x_category[:, i] for i in range(val_x_category.shape[1])] + [val_x_category_poly] + [val_x_embedding]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모델 구현"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "모델 구현에 대한 간단한 설명을 하겠습니다.  \n",
    "입력은 2개로 분리해서 생각하면 됩니다.  \n",
    "\n",
    "Wide 모델의 입력: category Feature을 Polynomial하게 바꿔준 데이터  \n",
    "Deep 모델의 입력: category Feature을 embeding 시켜준 데이터 + continuous한 데이터  \n",
    "\n",
    "그리고 출력은 lgbm 모델과 마찬가지로 1058개의 prediction 값이 row만큼 출력 됩니다.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_accuracy(y_true, y_pred):\n",
    "    true = K.equal(y_true, 1.0 ) \n",
    "    pred = K.greater(y_pred , 0.5)\n",
    "    true2 = K.cast(true , dtype = float)\n",
    "    pred2 = K.cast(pred , dtype = float)\n",
    "    return  K.sum(true2 * pred2) / K.sum(true2) \n",
    "\n",
    "gamma = 2.0\n",
    "epsilon = K.epsilon()\n",
    "\n",
    "def focal_loss(y_true, y_pred):\n",
    "    # https://www.kaggle.com/mathormad/resnet50-v2-keras-focal-loss-mix-up\n",
    "    pt = y_pred * y_true + (1-y_pred) * (1-y_true)\n",
    "    pt = K.clip(pt, epsilon, 1-epsilon)\n",
    "    CE = -K.log(pt)\n",
    "    FL = K.pow(1-pt, gamma) * CE\n",
    "    loss = K.sum(FL, axis=1)\n",
    "    return loss\n",
    "    return K.mean(K.sum(loss, axis=1))\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss=focal_loss   , # focal_loss  ,  # 'binary_crossentropy',\n",
    "              metrics=[ binary_accuracy ]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = \"/ckpt/my_checkpoint/KM-{epoch:04d}.ckpt\"\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(checkpoint_path,\n",
    "                                                 save_weights_only=True,\n",
    "                                                 save_best_only = True , \n",
    "                                                 save_freq = 'epoch' , \n",
    "                                                 verbose=1)\n",
    "# https://www.kaggle.com/rejpalcz/focalloss-for-keras\n",
    "def step_decay_schedule(initial_lr=1e-3, decay_factor=0.75, step_size=10):\n",
    "    '''\n",
    "    Wrapper function to create a LearningRateScheduler with step decay schedule.\n",
    "    '''\n",
    "    def schedule(epoch):\n",
    "        return initial_lr * (decay_factor ** np.floor(epoch/step_size))\n",
    "    \n",
    "    return tf.keras.callbacks.LearningRateScheduler(schedule)\n",
    "\n",
    "lr_sched = step_decay_schedule(initial_lr=1e-4, decay_factor=0.75, step_size=2)\n",
    "\n",
    "  # `val_loss`가 2번의 에포크에 걸쳐 향상되지 않으면 훈련을 멈춥니다.\n",
    "Early = tf.keras.callbacks.EarlyStopping(min_delta=0.0001, \n",
    "                                         patience=10 ,\n",
    "                                         monitor='val_loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 1000\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 임베딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "3289/3289 [==============================] - 14s 4ms/step - loss: 4.3020 - binary_accuracy: 0.0125 - val_loss: 1.8203 - val_binary_accuracy: 4.2691e-04\n",
      "Epoch 2/1000\n",
      "3289/3289 [==============================] - 13s 4ms/step - loss: 1.7199 - binary_accuracy: 0.0012 - val_loss: 1.6852 - val_binary_accuracy: 4.2691e-04\n",
      "Epoch 3/1000\n",
      "3289/3289 [==============================] - 13s 4ms/step - loss: 1.6064 - binary_accuracy: 0.0019 - val_loss: 1.6400 - val_binary_accuracy: 4.2691e-04\n",
      "Epoch 4/1000\n",
      "3289/3289 [==============================] - 13s 4ms/step - loss: 1.5613 - binary_accuracy: 0.0025 - val_loss: 1.6086 - val_binary_accuracy: 3.4153e-04\n",
      "Epoch 5/1000\n",
      "3289/3289 [==============================] - 13s 4ms/step - loss: 1.5227 - binary_accuracy: 0.0028 - val_loss: 1.6079 - val_binary_accuracy: 5.1230e-04\n",
      "Epoch 6/1000\n",
      "3289/3289 [==============================] - 13s 4ms/step - loss: 1.5048 - binary_accuracy: 0.0035 - val_loss: 1.5908 - val_binary_accuracy: 8.5383e-04\n",
      "Epoch 7/1000\n",
      "3289/3289 [==============================] - 14s 4ms/step - loss: 1.4837 - binary_accuracy: 0.0039 - val_loss: 1.5908 - val_binary_accuracy: 2.5615e-04\n",
      "Epoch 8/1000\n",
      "3289/3289 [==============================] - 13s 4ms/step - loss: 1.4730 - binary_accuracy: 0.0044 - val_loss: 1.5869 - val_binary_accuracy: 9.3921e-04\n",
      "Epoch 9/1000\n",
      "3289/3289 [==============================] - 14s 4ms/step - loss: 1.4601 - binary_accuracy: 0.0046 - val_loss: 1.5873 - val_binary_accuracy: 0.0014\n",
      "Epoch 10/1000\n",
      "3289/3289 [==============================] - 13s 4ms/step - loss: 1.4537 - binary_accuracy: 0.0048 - val_loss: 1.5860 - val_binary_accuracy: 7.6844e-04\n",
      "Epoch 11/1000\n",
      "3289/3289 [==============================] - 13s 4ms/step - loss: 1.4430 - binary_accuracy: 0.0050 - val_loss: 1.5867 - val_binary_accuracy: 0.0016\n",
      "Epoch 12/1000\n",
      "3289/3289 [==============================] - 13s 4ms/step - loss: 1.4379 - binary_accuracy: 0.0051 - val_loss: 1.5960 - val_binary_accuracy: 0.0011\n",
      "Epoch 13/1000\n",
      "3289/3289 [==============================] - 13s 4ms/step - loss: 1.4314 - binary_accuracy: 0.0052 - val_loss: 1.5929 - val_binary_accuracy: 0.0012\n",
      "Epoch 14/1000\n",
      "3289/3289 [==============================] - 13s 4ms/step - loss: 1.4273 - binary_accuracy: 0.0054 - val_loss: 1.5903 - val_binary_accuracy: 6.8306e-04\n",
      "Epoch 15/1000\n",
      "3289/3289 [==============================] - 14s 4ms/step - loss: 1.4219 - binary_accuracy: 0.0054 - val_loss: 1.5922 - val_binary_accuracy: 5.9768e-04\n",
      "Epoch 16/1000\n",
      "3289/3289 [==============================] - 14s 4ms/step - loss: 1.4191 - binary_accuracy: 0.0056 - val_loss: 1.5936 - val_binary_accuracy: 5.9768e-04\n",
      "Epoch 17/1000\n",
      "3289/3289 [==============================] - 14s 4ms/step - loss: 1.4157 - binary_accuracy: 0.0057 - val_loss: 1.5942 - val_binary_accuracy: 7.6844e-04\n",
      "Epoch 18/1000\n",
      "3289/3289 [==============================] - 14s 4ms/step - loss: 1.4131 - binary_accuracy: 0.0058 - val_loss: 1.5935 - val_binary_accuracy: 0.0012\n",
      "Epoch 19/1000\n",
      "3289/3289 [==============================] - 14s 4ms/step - loss: 1.4102 - binary_accuracy: 0.0059 - val_loss: 1.5925 - val_binary_accuracy: 0.0010\n",
      "Epoch 20/1000\n",
      "3289/3289 [==============================] - 14s 4ms/step - loss: 1.4095 - binary_accuracy: 0.0057 - val_loss: 1.5932 - val_binary_accuracy: 0.0013\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fce1cc2a190>"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(input_data, train_y, \n",
    "          epochs=epochs, \n",
    "          batch_size=batch_size, \n",
    "          validation_data=(val_data, val_y), \n",
    "          callbacks=[lr_sched, Early])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 새로운"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "3289/3289 [==============================] - 13s 4ms/step - loss: 4.1335 - binary_accuracy: 0.0112 - val_loss: 1.7965 - val_binary_accuracy: 4.2691e-04\n",
      "Epoch 2/1000\n",
      "3289/3289 [==============================] - 13s 4ms/step - loss: 1.7538 - binary_accuracy: 6.5559e-04 - val_loss: 1.6905 - val_binary_accuracy: 6.8306e-04\n",
      "Epoch 3/1000\n",
      "3289/3289 [==============================] - 13s 4ms/step - loss: 1.6523 - binary_accuracy: 9.3113e-04 - val_loss: 1.6490 - val_binary_accuracy: 2.5615e-04\n",
      "Epoch 4/1000\n",
      "3289/3289 [==============================] - 13s 4ms/step - loss: 1.6094 - binary_accuracy: 0.0012 - val_loss: 1.6451 - val_binary_accuracy: 8.5383e-05\n",
      "Epoch 5/1000\n",
      "3289/3289 [==============================] - 13s 4ms/step - loss: 1.5734 - binary_accuracy: 0.0014 - val_loss: 1.6351 - val_binary_accuracy: 4.2691e-04\n",
      "Epoch 6/1000\n",
      "3289/3289 [==============================] - 13s 4ms/step - loss: 1.5498 - binary_accuracy: 0.0019 - val_loss: 1.6255 - val_binary_accuracy: 5.1230e-04\n",
      "Epoch 7/1000\n",
      "3289/3289 [==============================] - 13s 4ms/step - loss: 1.5284 - binary_accuracy: 0.0022 - val_loss: 1.6278 - val_binary_accuracy: 3.4153e-04\n",
      "Epoch 8/1000\n",
      "3289/3289 [==============================] - 13s 4ms/step - loss: 1.5165 - binary_accuracy: 0.0026 - val_loss: 1.6275 - val_binary_accuracy: 4.2691e-04\n",
      "Epoch 9/1000\n",
      "3289/3289 [==============================] - 14s 4ms/step - loss: 1.5021 - binary_accuracy: 0.0026 - val_loss: 1.6218 - val_binary_accuracy: 4.2691e-04\n",
      "Epoch 10/1000\n",
      "3289/3289 [==============================] - 13s 4ms/step - loss: 1.4953 - binary_accuracy: 0.0030 - val_loss: 1.6284 - val_binary_accuracy: 3.4153e-04\n",
      "Epoch 11/1000\n",
      "3289/3289 [==============================] - 13s 4ms/step - loss: 1.4847 - binary_accuracy: 0.0032 - val_loss: 1.6221 - val_binary_accuracy: 4.2691e-04\n",
      "Epoch 12/1000\n",
      "3289/3289 [==============================] - 13s 4ms/step - loss: 1.4796 - binary_accuracy: 0.0034 - val_loss: 1.6338 - val_binary_accuracy: 5.1230e-04\n",
      "Epoch 13/1000\n",
      "3289/3289 [==============================] - 13s 4ms/step - loss: 1.4719 - binary_accuracy: 0.0038 - val_loss: 1.6342 - val_binary_accuracy: 4.2691e-04\n",
      "Epoch 14/1000\n",
      "3289/3289 [==============================] - 13s 4ms/step - loss: 1.4677 - binary_accuracy: 0.0037 - val_loss: 1.6344 - val_binary_accuracy: 4.2691e-04\n",
      "Epoch 15/1000\n",
      "3289/3289 [==============================] - 13s 4ms/step - loss: 1.4634 - binary_accuracy: 0.0037 - val_loss: 1.6364 - val_binary_accuracy: 4.2691e-04\n",
      "Epoch 16/1000\n",
      "3289/3289 [==============================] - 13s 4ms/step - loss: 1.4598 - binary_accuracy: 0.0040 - val_loss: 1.6357 - val_binary_accuracy: 3.4153e-04\n",
      "Epoch 17/1000\n",
      "3289/3289 [==============================] - 13s 4ms/step - loss: 1.4560 - binary_accuracy: 0.0041 - val_loss: 1.6354 - val_binary_accuracy: 4.2691e-04\n",
      "Epoch 18/1000\n",
      "3289/3289 [==============================] - 13s 4ms/step - loss: 1.4537 - binary_accuracy: 0.0042 - val_loss: 1.6376 - val_binary_accuracy: 3.4153e-04\n",
      "Epoch 19/1000\n",
      "3289/3289 [==============================] - 13s 4ms/step - loss: 1.4502 - binary_accuracy: 0.0043 - val_loss: 1.6353 - val_binary_accuracy: 3.4153e-04\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fb670225650>"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(input_data, train_y, \n",
    "          epochs=epochs, \n",
    "          batch_size=batch_size, \n",
    "          validation_data=(val_data, val_y), \n",
    "          callbacks=[lr_sched, Early])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 기존"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "3289/3289 [==============================] - 13s 4ms/step - loss: 3.7797 - binary_accuracy: 0.0134 - val_loss: 1.6372 - val_binary_accuracy: 0.0000e+00\n",
      "Epoch 2/1000\n",
      "3289/3289 [==============================] - 13s 4ms/step - loss: 1.6078 - binary_accuracy: 2.7554e-04 - val_loss: 1.5695 - val_binary_accuracy: 0.0000e+00\n",
      "Epoch 3/1000\n",
      "3289/3289 [==============================] - 13s 4ms/step - loss: 1.5426 - binary_accuracy: 5.9859e-04 - val_loss: 1.5531 - val_binary_accuracy: 0.0000e+00\n",
      "Epoch 4/1000\n",
      "3289/3289 [==============================] - 13s 4ms/step - loss: 1.5162 - binary_accuracy: 0.0012 - val_loss: 1.5525 - val_binary_accuracy: 0.0000e+00\n",
      "Epoch 5/1000\n",
      "3289/3289 [==============================] - 13s 4ms/step - loss: 1.4884 - binary_accuracy: 0.0025 - val_loss: 1.5560 - val_binary_accuracy: 1.7077e-04\n",
      "Epoch 6/1000\n",
      "3289/3289 [==============================] - 12s 4ms/step - loss: 1.4717 - binary_accuracy: 0.0030 - val_loss: 1.5591 - val_binary_accuracy: 1.7077e-04\n",
      "Epoch 7/1000\n",
      "3289/3289 [==============================] - 13s 4ms/step - loss: 1.4503 - binary_accuracy: 0.0037 - val_loss: 1.5682 - val_binary_accuracy: 1.7077e-04\n",
      "Epoch 8/1000\n",
      "3289/3289 [==============================] - 13s 4ms/step - loss: 1.4384 - binary_accuracy: 0.0043 - val_loss: 1.5760 - val_binary_accuracy: 2.5615e-04\n",
      "Epoch 9/1000\n",
      "3289/3289 [==============================] - 13s 4ms/step - loss: 1.4219 - binary_accuracy: 0.0049 - val_loss: 1.5836 - val_binary_accuracy: 3.4153e-04\n",
      "Epoch 10/1000\n",
      "3289/3289 [==============================] - 13s 4ms/step - loss: 1.4133 - binary_accuracy: 0.0056 - val_loss: 1.5895 - val_binary_accuracy: 1.7077e-04\n",
      "Epoch 11/1000\n",
      "3289/3289 [==============================] - 13s 4ms/step - loss: 1.4017 - binary_accuracy: 0.0061 - val_loss: 1.5946 - val_binary_accuracy: 2.5615e-04\n",
      "Epoch 12/1000\n",
      "3289/3289 [==============================] - 13s 4ms/step - loss: 1.3942 - binary_accuracy: 0.0064 - val_loss: 1.5992 - val_binary_accuracy: 1.7077e-04\n",
      "Epoch 13/1000\n",
      "3289/3289 [==============================] - 13s 4ms/step - loss: 1.3850 - binary_accuracy: 0.0068 - val_loss: 1.6018 - val_binary_accuracy: 1.7077e-04\n",
      "Epoch 14/1000\n",
      "3289/3289 [==============================] - 13s 4ms/step - loss: 1.3804 - binary_accuracy: 0.0071 - val_loss: 1.6057 - val_binary_accuracy: 1.7077e-04\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fd0cb1fef90>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(input_data, train_y, \n",
    "          epochs=epochs, \n",
    "          batch_size=batch_size, \n",
    "          validation_data=(val_data, val_y), \n",
    "          callbacks=[lr_sched, Early])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wide & Deep 모델에서는 구매 품목의 imbalance 문제를 해결하기 위해서 focal loss를 도입했습니다.  \n",
    "focal loss는 많은 비중을 차지해서 비교적 잘 분류되는 품목에는 영향력을 줄여주어서 분류가 잘 되지 않는 품목에 집중할 수 있도록 도와줍니다.  \n",
    "직접 아웃풋을 분석한 결과 Focal loss를 사용하기 전에는 구매율이 높은 품목들만 추천했다면, 도입한 뒤로는 좀 더 다양한 추천을 해주는 것 같습니다. 하지만 아직 정확도가 낮은 문제점이 있습니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train input data와 같은 방식으로 test data를 input 형식에 맞추어줌\n",
    "eval_input_data = [test_x_continue] + [test_x_category[:, i] for i in range(test_x_category.shape[1])] + [test_x_category_poly]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "193/193 [==============================] - 0s 2ms/step - loss: 1.6000 - binary_accuracy: 1.6192e-04\n"
     ]
    }
   ],
   "source": [
    "loss, acc = model.evaluate(eval_input_data, test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_loss: 1.6000030040740967 - test_acc: 0.00016191709437407553\n"
     ]
    }
   ],
   "source": [
    "print(f'test_loss: {loss} - test_acc: {acc}')\n",
    "# 문자열 앞에 f는 formating이었나? 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_y = model.predict(eval_input_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 정확도 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' \n",
    "pred_matrix: 예측된 아이템 행렬 파라미터.\n",
    "top_n: 상위 몇개를 추천으로 사용할 지 정하는 파라미터.\n",
    "test_matix: 고객ID와 Target이 있는 행렬 파라미터\n",
    "'''\n",
    "def get_acc(score_matrix, top_n, test_matix):\n",
    "    avg_acc = 0\n",
    "    for i in range(len(score_matrix)):\n",
    "        top = score_matrix.iloc[i].nlargest(top_n).index\n",
    "        tmp = 0\n",
    "        for j in range(len(top)):\n",
    "            if top[j] == test_matix[\"target\"][i]:\n",
    "                tmp += 1\n",
    "        acc = tmp / len(top)\n",
    "        avg_acc += acc / len(score_matrix)\n",
    "\n",
    "    return avg_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'clnt_id'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/envs/aiffel/lib/python3.7/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2894\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2895\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2896\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'clnt_id'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-41-cb2de8c8ad06>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdic\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'clnt_id'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtest_x\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'clnt_id'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'target'\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mtest_x\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'clac_nm2'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtarget_matrix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdic\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtarget_matrix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget_matrix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'index'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtarget_matrix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/aiffel/lib/python3.7/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2900\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2901\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2902\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2903\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2904\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/aiffel/lib/python3.7/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2895\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2896\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2897\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2898\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2899\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtolerance\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'clnt_id'"
     ]
    }
   ],
   "source": [
    "dic = {'clnt_id': test_x['clnt_id'],'target' : test_x['clac_nm2']}\n",
    "target_matrix = pd.DataFrame(dic).reset_index()\n",
    "target_matrix = target_matrix.drop(['index'], axis=1)\n",
    "\n",
    "target_matrix.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>1045</th>\n",
       "      <th>1046</th>\n",
       "      <th>1047</th>\n",
       "      <th>1048</th>\n",
       "      <th>1049</th>\n",
       "      <th>1050</th>\n",
       "      <th>1051</th>\n",
       "      <th>1052</th>\n",
       "      <th>1053</th>\n",
       "      <th>1054</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.003870</td>\n",
       "      <td>0.009187</td>\n",
       "      <td>0.013846</td>\n",
       "      <td>0.001066</td>\n",
       "      <td>0.050542</td>\n",
       "      <td>0.012555</td>\n",
       "      <td>0.017187</td>\n",
       "      <td>0.040968</td>\n",
       "      <td>0.066419</td>\n",
       "      <td>0.032210</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001556</td>\n",
       "      <td>0.003507</td>\n",
       "      <td>0.001765</td>\n",
       "      <td>0.006174</td>\n",
       "      <td>0.009276</td>\n",
       "      <td>0.002257</td>\n",
       "      <td>0.054420</td>\n",
       "      <td>0.004990</td>\n",
       "      <td>0.016043</td>\n",
       "      <td>0.002339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.015519</td>\n",
       "      <td>0.007073</td>\n",
       "      <td>0.029056</td>\n",
       "      <td>0.004541</td>\n",
       "      <td>0.086528</td>\n",
       "      <td>0.004747</td>\n",
       "      <td>0.049089</td>\n",
       "      <td>0.037475</td>\n",
       "      <td>0.037094</td>\n",
       "      <td>0.049752</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002761</td>\n",
       "      <td>0.001996</td>\n",
       "      <td>0.004467</td>\n",
       "      <td>0.002363</td>\n",
       "      <td>0.027529</td>\n",
       "      <td>0.002927</td>\n",
       "      <td>0.103238</td>\n",
       "      <td>0.018922</td>\n",
       "      <td>0.008862</td>\n",
       "      <td>0.002350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.001252</td>\n",
       "      <td>0.048368</td>\n",
       "      <td>0.045424</td>\n",
       "      <td>0.000945</td>\n",
       "      <td>0.090145</td>\n",
       "      <td>0.016341</td>\n",
       "      <td>0.086308</td>\n",
       "      <td>0.094145</td>\n",
       "      <td>0.137432</td>\n",
       "      <td>0.054816</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001171</td>\n",
       "      <td>0.003011</td>\n",
       "      <td>0.005142</td>\n",
       "      <td>0.001550</td>\n",
       "      <td>0.045851</td>\n",
       "      <td>0.002800</td>\n",
       "      <td>0.096974</td>\n",
       "      <td>0.003262</td>\n",
       "      <td>0.068804</td>\n",
       "      <td>0.004877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.022368</td>\n",
       "      <td>0.008663</td>\n",
       "      <td>0.030572</td>\n",
       "      <td>0.006060</td>\n",
       "      <td>0.073585</td>\n",
       "      <td>0.004553</td>\n",
       "      <td>0.042916</td>\n",
       "      <td>0.029611</td>\n",
       "      <td>0.041933</td>\n",
       "      <td>0.058360</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001998</td>\n",
       "      <td>0.001469</td>\n",
       "      <td>0.003201</td>\n",
       "      <td>0.002157</td>\n",
       "      <td>0.029728</td>\n",
       "      <td>0.002877</td>\n",
       "      <td>0.108355</td>\n",
       "      <td>0.019072</td>\n",
       "      <td>0.006630</td>\n",
       "      <td>0.002142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.060900</td>\n",
       "      <td>0.062453</td>\n",
       "      <td>0.007792</td>\n",
       "      <td>0.026579</td>\n",
       "      <td>0.022882</td>\n",
       "      <td>0.006127</td>\n",
       "      <td>0.009927</td>\n",
       "      <td>0.007433</td>\n",
       "      <td>0.020697</td>\n",
       "      <td>0.028880</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004389</td>\n",
       "      <td>0.015503</td>\n",
       "      <td>0.005152</td>\n",
       "      <td>0.001835</td>\n",
       "      <td>0.021648</td>\n",
       "      <td>0.001776</td>\n",
       "      <td>0.030017</td>\n",
       "      <td>0.009923</td>\n",
       "      <td>0.079559</td>\n",
       "      <td>0.000965</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 1055 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       0         1         2         3         4         5         6     \\\n",
       "0  0.003870  0.009187  0.013846  0.001066  0.050542  0.012555  0.017187   \n",
       "1  0.015519  0.007073  0.029056  0.004541  0.086528  0.004747  0.049089   \n",
       "2  0.001252  0.048368  0.045424  0.000945  0.090145  0.016341  0.086308   \n",
       "3  0.022368  0.008663  0.030572  0.006060  0.073585  0.004553  0.042916   \n",
       "4  0.060900  0.062453  0.007792  0.026579  0.022882  0.006127  0.009927   \n",
       "\n",
       "       7         8         9     ...      1045      1046      1047      1048  \\\n",
       "0  0.040968  0.066419  0.032210  ...  0.001556  0.003507  0.001765  0.006174   \n",
       "1  0.037475  0.037094  0.049752  ...  0.002761  0.001996  0.004467  0.002363   \n",
       "2  0.094145  0.137432  0.054816  ...  0.001171  0.003011  0.005142  0.001550   \n",
       "3  0.029611  0.041933  0.058360  ...  0.001998  0.001469  0.003201  0.002157   \n",
       "4  0.007433  0.020697  0.028880  ...  0.004389  0.015503  0.005152  0.001835   \n",
       "\n",
       "       1049      1050      1051      1052      1053      1054  \n",
       "0  0.009276  0.002257  0.054420  0.004990  0.016043  0.002339  \n",
       "1  0.027529  0.002927  0.103238  0.018922  0.008862  0.002350  \n",
       "2  0.045851  0.002800  0.096974  0.003262  0.068804  0.004877  \n",
       "3  0.029728  0.002877  0.108355  0.019072  0.006630  0.002142  \n",
       "4  0.021648  0.001776  0.030017  0.009923  0.079559  0.000965  \n",
       "\n",
       "[5 rows x 1055 columns]"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "small_p = pd.DataFrame(predict_y[:1000])\n",
    "small_p.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "정확도: 2.0999999999999983%\n"
     ]
    }
   ],
   "source": [
    "accuracy  = get_acc(small_p, 5, target_matrix)\n",
    "\n",
    "print(f\"정확도: {accuracy*100}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 정확도 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pred_list(predict_y, top_n, target_matrix, columns):\n",
    "    test_matrix = target_matrix.copy()\n",
    "    pred_matrix = predict_y.copy()\n",
    "    pred_matrix.rename(columns = columns['hangle'], inplace = True)\n",
    "    for i in range(len(pred_matrix)):\n",
    "        top = pred_matrix.iloc[i].nlargest(top_n).index\n",
    "        top = pd.DataFrame(top.astype(str).to_frame().apply(lambda x: \", \".join(x)))\n",
    "        test_matrix.loc[i, 'pred'] = top.values\n",
    "    test_matrix['target'] = test_matrix['target'].apply(lambda x: columns['hangle'][x])\n",
    "    return test_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "d={'hangle': Input['clac_nm2'], 'label': data['clac_nm2']}\n",
    "df = pd.DataFrame(data=d).drop_duplicates()\n",
    "cate2papago = df.set_index('label').to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = get_pred_list(small_p, 5, target_matrix, cate2papago)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clnt_id</th>\n",
       "      <th>target</th>\n",
       "      <th>pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>42301</td>\n",
       "      <td>여자 로퍼</td>\n",
       "      <td>[여자 골프 의류 세트, 여성 가죽 의류, 키즈 우산, 스포츠 가방, 치즈'!]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>29233</td>\n",
       "      <td>제너럴 요구르트</td>\n",
       "      <td>[즉석 죽, 포도, 국내 Beefs-Rounds, 옥수수 스낵, 우유]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>58449</td>\n",
       "      <td>밤</td>\n",
       "      <td>[포도, 아기 매트리스 패드, 냉동 떡볶이, 냉동 튀김 식품, 라면]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>48924</td>\n",
       "      <td>밤</td>\n",
       "      <td>[옥수수 스낵, 즉석 죽, 제너럴 티 드링크, 국내 Beefs-Rounds, 포도]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>47681</td>\n",
       "      <td>여자 로퍼</td>\n",
       "      <td>[여자 로퍼, 여성 청바지, 펫 도그 푸드, 조리 기구 세트, 여성 가죽 의류]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>16419</td>\n",
       "      <td>여성 스웨터 / 풀오버</td>\n",
       "      <td>[남성 정장, 여성 청바지, 기타 컴퓨터 액세서리, 다른 영양학적 Supplemen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>70886</td>\n",
       "      <td>팬케이크 믹스</td>\n",
       "      <td>[옥수수 스낵, 국내 Beefs-Rounds, 라면, 즉석 죽, 제너럴 티 드링크]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>16419</td>\n",
       "      <td>여성 스웨터 / 풀오버</td>\n",
       "      <td>[남성 정장, 여성 청바지, 기타 컴퓨터 액세서리, 다른 영양학적 Supplemen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>47330</td>\n",
       "      <td>기능성 우유</td>\n",
       "      <td>[즉석 죽, 옥수수 스낵, 포도, 일반 스낵, 라면]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>13871</td>\n",
       "      <td>인공조미료</td>\n",
       "      <td>[즉석 죽, 포도, 국내 Porks-Picnics, 라면, 옥수수 스낵]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     clnt_id        target                                               pred\n",
       "0      42301         여자 로퍼       [여자 골프 의류 세트, 여성 가죽 의류, 키즈 우산, 스포츠 가방, 치즈'!]\n",
       "1      29233      제너럴 요구르트            [즉석 죽, 포도, 국내 Beefs-Rounds, 옥수수 스낵, 우유]\n",
       "2      58449             밤             [포도, 아기 매트리스 패드, 냉동 떡볶이, 냉동 튀김 식품, 라면]\n",
       "3      48924             밤     [옥수수 스낵, 즉석 죽, 제너럴 티 드링크, 국내 Beefs-Rounds, 포도]\n",
       "4      47681         여자 로퍼       [여자 로퍼, 여성 청바지, 펫 도그 푸드, 조리 기구 세트, 여성 가죽 의류]\n",
       "..       ...           ...                                                ...\n",
       "995    16419  여성 스웨터 / 풀오버  [남성 정장, 여성 청바지, 기타 컴퓨터 액세서리, 다른 영양학적 Supplemen...\n",
       "996    70886       팬케이크 믹스     [옥수수 스낵, 국내 Beefs-Rounds, 라면, 즉석 죽, 제너럴 티 드링크]\n",
       "997    16419  여성 스웨터 / 풀오버  [남성 정장, 여성 청바지, 기타 컴퓨터 액세서리, 다른 영양학적 Supplemen...\n",
       "998    47330        기능성 우유                      [즉석 죽, 옥수수 스낵, 포도, 일반 스낵, 라면]\n",
       "999    13871         인공조미료           [즉석 죽, 포도, 국내 Porks-Picnics, 라면, 옥수수 스낵]\n",
       "\n",
       "[1000 rows x 3 columns]"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test[test['pred'].notna()]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aiffel",
   "language": "python",
   "name": "aiffel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
