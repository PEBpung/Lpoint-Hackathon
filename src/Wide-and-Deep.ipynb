{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import argparse\n",
    "from sklearn.preprocessing import OneHotEncoder, MinMaxScaler, StandardScaler\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import Input, concatenate, Embedding, Reshape\n",
    "from tensorflow.keras.layers import Flatten, concatenate, Lambda, Dropout\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.regularizers import l2, l1_l2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터를 호출하거나 없으면 다운 받는 함수\n",
    "def maybe_download(train_data, test_data):\n",
    "    '''만약 adult data가 없다면 다운 받으세요'''\n",
    "    \n",
    "    ## 여기서 다양한 feature을 사용하는 것 보니까 Lpoint 데이터도 이런 형식으로 가능할듯.\n",
    "    COLUMNS = [\"age\", \"workclass\", \"fnlwgt\", \"education\", \"education_num\",\n",
    "               \"marital_status\", \"occupation\", \"relationship\", \"race\", \"gender\",\n",
    "               \"capital_gain\", \"capital_loss\", \"hours_per_week\", \"native_country\",\n",
    "               \"income_bracket\"]\n",
    "    \n",
    "    if not os.path.exists(train_data):\n",
    "        print('training data를 다운로드 합니다....')\n",
    "        df_train = pd.read_csv(\"http://mlr.cs.umass.edu/ml/machine-learning-databases/adult/adult.data\",\n",
    "            names=COLUMNS, skipinitialspace=True)\n",
    "    else: \n",
    "        df_train = pd.read_csv('train.csv')\n",
    "            \n",
    "    if not os.path.exists(test_data):\n",
    "        print('testing data를 다운로드 합니다....')\n",
    "        df_test = pd.read_csv(\"http://mlr.cs.umass.edu/ml/machine-learning-databases/adult/adult.test\",\n",
    "            names=COLUMNS, skipinitialspace=True, skiprows=1) \n",
    "    else:\n",
    "        df_test = pd.read_csv('test.csv')\n",
    "        \n",
    "    return df_train, df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_columns(x_cols):\n",
    "    '''pandas dataframe에서 crossed columns를 쉽게 만들어주는 함수\n",
    "    colomn list를 dict 형식으로 만들어 준다'''\n",
    "    \n",
    "    crossed_columns = dict()\n",
    "    colnames = ['_'.join(x_c) for x_c in x_cols]\n",
    "    for cname, x_c in zip(colnames, x_cols):\n",
    "        crossed_columns[cname] = x_c\n",
    "    return crossed_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'a_b': ['a', 'b'], 'c_d': ['c', 'd']}\n"
     ]
    }
   ],
   "source": [
    "# 예시\n",
    "test= [['a', 'b'], \n",
    "       ['c', 'd']]\n",
    "print(cross_columns(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def val2inx(df, cols):\n",
    "    ''' 카테고리 columns를 임베딩 하기 전에 index로 만들어준다.'''\n",
    "    val_type = dict()\n",
    "    for c in cols:\n",
    "        val_types[c] = df[c].unique()\n",
    "        \n",
    "    val_to_idx = dict()\n",
    "    for k, v in val_types.items():\n",
    "        val_to_idx[k] = {o: i for i, o in enumerate(val_types[k])}\n",
    "        \n",
    "    for k, v in val_to_idx.items():\n",
    "        df[k] = df[k].apply(lambda x: v[x])\n",
    "        \n",
    "    unique_vals = dict()\n",
    "    for c in cols:\n",
    "        unique_vals[c] = df[c].nunique()\n",
    "        \n",
    "    return df, unique_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def onehot(x):\n",
    "    '''onehotencoding 후 행렬로 변환'''\n",
    "    return np.array(OneHotEncoder().fit_transform(x).todense())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![img](\"../img/model_structure.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embeddin_input(name, n_in, n_out, reg):\n",
    "    inp = Input(shape(1, ), dtype='int64', name=name)\n",
    "    return inp, Embedding(n_in, n_out, input_length=1, embeddins_regularizer=l2(reg)(inp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def continous_input(name):\n",
    "    inp = Input(shape(1,), dtype='float32', name=name)\n",
    "    return inp, Reshape((1,1))(inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wide(df_train, df_test, wide_cols, x_cols, target, model_type, method):\n",
    "    '''Run the wide (linear) model\n",
    "    \n",
    "    Params:\n",
    "    ----\n",
    "    wide_cols    : wide model에 맞게 사용되는 columns\n",
    "    x_cols       : crossed에 맞게 사용되는 columns\n",
    "    target       : the target feature\n",
    "    model_type   : wide 와 wide_deep 모델 둘다 수용. 만약 'wide_depp'이라면 \n",
    "                   build 하고 inputs를 반환한다. 나머지는 안됨\n",
    "    method       : regression, logistic, multiclass 중 선택\n",
    "    \n",
    "    Returns:\n",
    "    -----\n",
    "    if 'wide':\n",
    "        test set을 얻은 결과를 print 한다. \n",
    "    if 'wide_deep':\n",
    "        X_train, y_train, X_test, y_test: the inputs required to build wide and deep\n",
    "    '''\n",
    "    \n",
    "    df_train['IS_TRAIN'] = 1\n",
    "    df_test['IS_TRAIN'] = 0\n",
    "    df_wide = pd.concat([df_train, df_test])\n",
    "    \n",
    "# 여기서 crossed_columns란 무엇을 의미할까?\n",
    "    crossed_columns_d = cross_columns(x_cols)\n",
    "    categorical_columns = list(\n",
    "        df_wide.select_dtypes(include=['object']).columns)\n",
    "    \n",
    "    wide_cols += list(crossed_columns_d.keys())\n",
    "    \n",
    "    for k, v in crossed_columns_d.items():\n",
    "        df_wide[k] = df_wide[v].apply(lambda x: '-'.join(x), axis=1)\n",
    "    \n",
    "    df_wide = df_wide[wide_cols + [target] + ['IS_TRAIN']]\n",
    "    \n",
    "    dummy_cols = [\n",
    "        c for c in wide_cols if c in categorical_columns + list(crossed_columns_d.key())]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aiffel",
   "language": "python",
   "name": "aiffel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
